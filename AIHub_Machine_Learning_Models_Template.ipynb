{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o6_9Bv9c5y5A",
        "Ik9YlluW53uW",
        "D_KjYWx0LF77",
        "2j_CUYng53ry",
        "Ey-dUssz53pH",
        "QfJcnEpaLPWw",
        "IFMC96Ce53ms",
        "YIfSXe7kLq6Z",
        "rK5VMMGzLvLp",
        "yGtOrauGLx1r",
        "nlzhKt5cL1Qh",
        "cglhVPi9L7HE",
        "0652UpKXMAgi",
        "2wtMxt6EMGuD",
        "coiFF3f3MJUr",
        "suZYR9DeMMFz",
        "eA3vwXzhMUkm",
        "32gtrZJEMXoW"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDMGW3WlyYgIO9SHKo0yj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariomvyas/AIhub/blob/main/AIHub_Machine_Learning_Models_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIHub Machine Learning Models Template by Hariom Vyas"
      ],
      "metadata": {
        "id": "G8jCMIhT5tvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning Models\n",
        "\n",
        "These are the models where the algorithm is trained using labeled data (data that has an outcome variable or label), with the aim of predicting the outcome of new, unseen data. Examples of supervised learning models include linear regression, logistic regression, decision trees, random forests, support vector machines, and artificial neural networks."
      ],
      "metadata": {
        "id": "jf8FfMNI58Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "o6_9Bv9c5y5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92lIOzNf5kry"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "Ik9YlluW53uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "K4k7Vl5T6Chp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees"
      ],
      "metadata": {
        "id": "D_KjYWx0LF77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "OeZby9XRLJjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest model"
      ],
      "metadata": {
        "id": "2j_CUYng53ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "gcxeUcRv6HSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "Ey-dUssz53pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "AQcr4uIj6K-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "QfJcnEpaLPWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "nqSLgBkDLOq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "IFMC96Ce53ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define and train the model\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "jbh1N0JL6PTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Networks (Multilayer Perceptron)"
      ],
      "metadata": {
        "id": "YIfSXe7kLq6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "4546i-EVLqb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Machines (GBM)"
      ],
      "metadata": {
        "id": "rK5VMMGzLvLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "ufZEd0PJLxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extreme Gradient Boosting (XGBoost)"
      ],
      "metadata": {
        "id": "yGtOrauGLx1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "VXXptEqCL0od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM"
      ],
      "metadata": {
        "id": "nlzhKt5cL1Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "d_train = lgb.Dataset(X_train, label=y_train)\n",
        "params = {'objective': 'binary', 'metric': 'binary_logloss'}\n",
        "model = lgb.train(params, d_train, 100)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = [1 if x >= 0.5 else 0 for x in y_pred]\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "OSGLTF65L6JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost"
      ],
      "metadata": {
        "id": "cglhVPi9L7HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import catboost as cb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = cb.CatBoostClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "pYlc_puVL6r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost"
      ],
      "metadata": {
        "id": "0652UpKXMAgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = AdaBoostClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "1qms75dBMAL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression"
      ],
      "metadata": {
        "id": "tfRGn26CMDj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = RidgeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy, accuracy)"
      ],
      "metadata": {
        "id": "nVGUy5oEMDXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso Regression"
      ],
      "metadata": {
        "id": "2wtMxt6EMGuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "yboQoJbUMGgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elastic Net"
      ],
      "metadata": {
        "id": "coiFF3f3MJUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "X_1LC9D2MLVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Regression"
      ],
      "metadata": {
        "id": "suZYR9DeMMFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = BayesianRidge()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "Wc5y_GTAMJID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian Processes"
      ],
      "metadata": {
        "id": "38LyUwTZMOT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "kernel = RBF(1.0)\n",
        "model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "id": "njc3sgCeMOGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Discriminant Analysis (LDA)"
      ],
      "metadata": {
        "id": "eA3vwXzhMUkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = LinearDiscriminantAnalysis()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "GaZMO4GAMUQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quadratic Discriminant Analysis (QDA)"
      ],
      "metadata": {
        "id": "32gtrZJEMXoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model = QuadraticDiscriminantAnalysis()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "lEKbms1MMXeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning Models\n",
        "\n",
        "These are the models where the algorithm is trained using unlabeled data (data that does not have an outcome variable or label), with the aim of discovering patterns, relationships, or clusters in the data. Examples of unsupervised learning models include k-means clustering, hierarchical clustering, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE)."
      ],
      "metadata": {
        "id": "8bFIVttP53bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-means Clustering"
      ],
      "metadata": {
        "id": "F3shmnVQP5yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-means Clustering with Elbow Method"
      ],
      "metadata": {
        "id": "E1oE2fwkP_pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Scale the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Step 4: Find the optimal number of clusters using the elbow method\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(data_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Train the model\n",
        "n_clusters = 3 # set the number of clusters based on the elbow method\n",
        "model = KMeans(n_clusters=n_clusters)\n",
        "model.fit(data_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "labels = model.labels_\n"
      ],
      "metadata": {
        "id": "kkIbmZGqP6in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in Step 4 of the K-means Clustering with the Elbow Method, we are using the Within-Cluster Sum of Squares (WCSS) to determine the optimal number of clusters. The Elbow Method involves plotting the WCSS values for different values of K and looking for an \"elbow\" in the plot where the WCSS begins to level off. The number of clusters associated with the elbow point is then chosen as the optimal number of clusters."
      ],
      "metadata": {
        "id": "kdLfsmY4RgC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### K-means Clustering with Silhouette Method"
      ],
      "metadata": {
        "id": "qgq_7mZcQHA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Load the data\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 3: Scale the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Step 4: Find the optimal number of clusters using the silhouette method\n",
        "silhouette_scores = []\n",
        "for n_clusters in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=n_clusters)\n",
        "    labels = kmeans.fit_predict(data_scaled)\n",
        "    silhouette_avg = silhouette_score(data_scaled, labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
        "\n",
        "plt.plot(range(2, 11), silhouette_scores)\n",
        "plt.title('Silhouette Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Train the model\n",
        "n_clusters = optimal_n_clusters # set the number of clusters based on the silhouette method\n",
        "model = KMeans(n_clusters=n_clusters)\n",
        "model.fit(data_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "labels = model.labels_"
      ],
      "metadata": {
        "id": "QYVBktBZQG28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Step 4 of the K-means Clustering with the Silhouette Method, we are using the Silhouette Score to determine the optimal number of clusters. The Silhouette Score measures the similarity of each data point to its own cluster compared to other clusters. A higher Silhouette Score indicates that the data point is better matched to its own cluster than to neighboring clusters. The optimal number of clusters is chosen as the number of clusters associated with the highest Silhouette Score."
      ],
      "metadata": {
        "id": "PL-djCPoRjU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical clustering"
      ],
      "metadata": {
        "id": "zhwymfTdQf_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Perform hierarchical clustering using ward linkage and euclidean distance\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
        "\n",
        "# Fit the hierarchical clustering model using the appropriate number of clusters\n",
        "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "y_hc = hc.fit_predict(X)\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.title('Hierarchical Clustering')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aANouBrzQfxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template performs hierarchical clustering using the ward linkage method and euclidean distance. The appropriate number of clusters is determined based on the dendrogram plot. Finally, the clusters are visualized in a scatter plot."
      ],
      "metadata": {
        "id": "euujIPslRnQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "SS2xcQUzQmsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Perform PCA with n_components=2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualize the data in the reduced dimensions\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qu_KKHnWQmfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template performs PCA on a dataset with two features. First, the data is standardized using StandardScaler. Then, PCA is performed with n_components=2. Finally, the data is visualized in the reduced dimensions."
      ],
      "metadata": {
        "id": "KxEiSl2JR8l3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Independent Component Analysis (ICA)"
      ],
      "metadata": {
        "id": "nHKqymOwQpvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Perform ICA with n_components=2\n",
        "ica = FastICA(n_components=2)\n",
        "X_ica = ica.fit_transform(X)\n",
        "\n",
        "# Visualize the data in the independent components\n",
        "plt.scatter(X_ica[:, 0], X_ica[:, 1])\n",
        "plt.xlabel('Independent Component 1')\n",
        "plt.ylabel('Independent Component 2')\n",
        "plt.title('ICA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DaaN2JXlQpl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template performs ICA on a dataset with two features. ICA is performed with n_components=2. Finally, the data is visualized in the independent components."
      ],
      "metadata": {
        "id": "5U3U4WLVSL3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-Distributed Stochastic Neighbor Embedding (t-SNE)"
      ],
      "metadata": {
        "id": "-L_7t4-YQseu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Perform t-SNE with n_components=2 and perplexity=30\n",
        "tsne = TSNE(n_components=2, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Visualize the data in the reduced dimensions\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('t-SNE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uv1PyeyRQsSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template performs t-SNE on a dataset with two features. t-SNE is performed with n_components=2 and perplexity=30. Finally, the data is visualized in the reduced dimensions."
      ],
      "metadata": {
        "id": "52Rs_JMzSWuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoders"
      ],
      "metadata": {
        "id": "L-brNW5MQxnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (X.shape[1],)\n",
        "\n",
        "# Define the encoder\n",
        "input_layer = Input(shape=input_shape)\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "encoded = Dense(16, activation='relu')(encoded)\n",
        "\n",
        "# Define the decoder\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(64, activation='relu')(decoded)\n",
        "decoded = Dense(X.shape[1], activation='linear')(decoded)\n",
        "\n",
        "# Define the autoencoder\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X, X, epochs=50, batch_size=32, shuffle=True)\n",
        "\n",
        "# Use the encoder to get the encoded representation of the data\n",
        "encoder = Model(input_layer, encoded)\n",
        "X_encoded = encoder.predict(X)\n",
        "\n",
        "# Visualize the data in the encoded dimensions\n",
        "plt.scatter(X_encoded[:, 0], X_encoded[:, 1])\n",
        "plt.xlabel('Encoded Dimension 1')\n",
        "plt.ylabel('Encoded Dimension 2')\n",
        "plt.title('Autoencoder')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ouip0kC0Qxe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template defines an autoencoder with 3 encoding layers and 3 decoding layers. The autoencoder is trained on the input data, and then the encoder is used to get the encoded representation of the data. Finally, the data is visualized in the encoded dimensions."
      ],
      "metadata": {
        "id": "bGN6MSMXSk0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian Mixture Models (GMM)"
      ],
      "metadata": {
        "id": "OTFzJcdXQ16K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Define the number of clusters\n",
        "n_clusters = 3\n",
        "\n",
        "# Initialize the Gaussian Mixture Model\n",
        "gmm = GaussianMixture(n_components=n_clusters)\n",
        "\n",
        "# Fit the GMM on the data\n",
        "gmm.fit(X)\n",
        "\n",
        "# Get the labels for each data point\n",
        "labels = gmm.predict(X)\n",
        "\n",
        "# Visualize the data with different colors for each cluster\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Gaussian Mixture Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hGVz7gUXQ1wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template defines a Gaussian Mixture Model with n_clusters=3. The model is fit on the data, and then the labels are obtained for each data point. Finally, the data is visualized with different colors for each cluster."
      ],
      "metadata": {
        "id": "jTGE6S_iSzD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anomaly detection models (e.g. one-class SVM)"
      ],
      "metadata": {
        "id": "CPDITcAmQ6zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, [2, 3]].values\n",
        "\n",
        "# Define the nu parameter\n",
        "nu = 0.05\n",
        "\n",
        "# Initialize the One-Class SVM model\n",
        "ocsvm = OneClassSVM(nu=nu)\n",
        "\n",
        "# Fit the model on the data\n",
        "ocsvm.fit(X)\n",
        "\n",
        "# Get the predictions for each data point\n",
        "y_pred = ocsvm.predict(X)\n",
        "\n",
        "# Visualize the data with different colors for inliers and outliers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X[y_pred==1, 0], X[y_pred==1, 1], c='blue', label='inliers')\n",
        "plt.scatter(X[y_pred==-1, 0], X[y_pred==-1, 1], c='red', label='outliers')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('One-Class SVM Anomaly Detection')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-u7L9hrUQ6jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template defines a one-class SVM anomaly detection model with nu=0.05. The model is fit on the data, and then the predictions are obtained for each data point. Finally, the data is visualized with different colors for inliers and outliers."
      ],
      "metadata": {
        "id": "wII-62ySS_Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-Organizing Maps (SOM)"
      ],
      "metadata": {
        "id": "sDonLTJ7Q_Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from minisom import MiniSom\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Extract the features\n",
        "X = data.iloc[:, :-1].values\n",
        "\n",
        "# Define the SOM parameters\n",
        "input_len = X.shape[1]\n",
        "map_size = (10, 10)\n",
        "sigma = 1.0\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Initialize the SOM model\n",
        "som = MiniSom(map_size[0], map_size[1], input_len, sigma=sigma, learning_rate=learning_rate)\n",
        "\n",
        "# Train the SOM model\n",
        "som.random_weights_init(X)\n",
        "som.train_random(X, 100)\n",
        "\n",
        "# Visualize the SOM model\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.pcolor(som.distance_map().T, cmap='bone_r')\n",
        "plt.colorbar()\n",
        "\n",
        "# Add markers for the data points\n",
        "markers = ['o', 's']\n",
        "colors = ['C0', 'C1']\n",
        "for i, x in enumerate(X):\n",
        "    w = som.winner(x)\n",
        "    plt.plot(w[0]+0.5, w[1]+0.5, markers[y[i]], markerfacecolor='None',\n",
        "             markeredgecolor=colors[y[i]], markersize=10, markeredgewidth=2)\n",
        "\n",
        "plt.title('Self-Organizing Map')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4WmfVr0WQ_GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template defines a Self-Organizing Map (SOM) with a 10x10 grid and a sigma of 1.0 and a learning_rate of 0.5. The model is trained on the data for 100 epochs. The SOM is then visualized, with markers added for each data point. The markers are colored based on their corresponding class."
      ],
      "metadata": {
        "id": "hDSVd2agTRTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note that this is not an exhaustive list and there are many other unsupervised learning models.**"
      ],
      "metadata": {
        "id": "UH6l_0AGQP_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-supervised Learning Models\n",
        "\n",
        "These are the models that use a combination of labeled and unlabeled data for training. This approach is used when there is limited labeled data available or labeling the data is expensive. Examples of semi-supervised learning models include self-training, co-training, and multi-view learning."
      ],
      "metadata": {
        "id": "o6TIm0iI53Ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning Models\n",
        "\n",
        "These are the models where the algorithm learns by interacting with an environment and receiving feedback in the form of rewards or penalties. The goal of reinforcement learning is to learn a policy (a set of rules) that maximizes the cumulative reward over time. Examples of reinforcement learning models include Q-learning, SARSA, and deep reinforcement learning."
      ],
      "metadata": {
        "id": "bb20zQ6P53Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Models\n",
        "\n",
        "These are the models that use artificial neural networks with multiple layers to learn hierarchical representations of the data. Deep learning models are particularly good at handling complex, high-dimensional data, such as images, audio, and natural language text. Examples of deep learning models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer models."
      ],
      "metadata": {
        "id": "NQ49IqPX6xVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other types of models\n",
        "\n",
        "Other types of machine learning models include Bayesian models, decision-theoretic models, and ensemble models (e.g., bagging, boosting, and stacking)."
      ],
      "metadata": {
        "id": "bMQz9qOG7hIY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ttRik1bG7kv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}