{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7m+D6THxL9mWHdhEan3cR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariomvyas/AIhub/blob/main/AIHub_Data_Cleaning_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIHub Data Cleaning Template by Hariom Vyas\n",
        "\n",
        "Data cleaning is an essential step in preparing data for machine learning. Here are the steps you can follow in Python for data cleaning:"
      ],
      "metadata": {
        "id": "mduORSRB1QA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import libraries: \n",
        "First, you need to import the necessary libraries such as pandas, numpy, and sklearn."
      ],
      "metadata": {
        "id": "-MPjqc7s1X1q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yxtn8Dcq0_B8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the data \n",
        "Load the dataset you want to clean using pandas."
      ],
      "metadata": {
        "id": "1i8rBmJS1jYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('filename.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "JJ2QvxYry8yk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Handle missing values: \n",
        "Check for missing values in your dataset and handle them appropriately. You can either remove rows or columns with missing values or impute the missing values using techniques such as mean, median, or mode."
      ],
      "metadata": {
        "id": "q7CstRaD199X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Impute missing values\n",
        "data.fillna(data.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "BFxz7gXL2EV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Handle categorical data:\n",
        "Convert categorical data to numerical data using techniques such as one-hot encoding or label encoding."
      ],
      "metadata": {
        "id": "ouHs0VNL1966"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "data = pd.get_dummies(data, columns=['category'])\n",
        "\n",
        "# Label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "data['category'] = le.fit_transform(data['category'])"
      ],
      "metadata": {
        "id": "qaCNXgDI2KlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove duplicate values:\n",
        "Remove duplicate rows from the dataset."
      ],
      "metadata": {
        "id": "4V5gVVTg194l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "C7kKbWkh2T0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Standardize the data:\n",
        "Standardize the data to ensure that all features are on the same scale."
      ],
      "metadata": {
        "id": "a29GewV-192I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "7_UiK2lj2ad6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Normalize the data:\n",
        "Normalize the data to ensure that all features have the same importance."
      ],
      "metadata": {
        "id": "hfKo54v119zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "data_normalized = normalize(data, norm='l2')"
      ],
      "metadata": {
        "id": "ZLxRN8YM2hjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Feature selection: \n",
        "Select the relevant features for your model."
      ],
      "metadata": {
        "id": "tIRn7ARx19xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features using feature importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(data, target)\n",
        "feat_importances = pd.Series(rf.feature_importances_, index=data.columns)\n",
        "feat_importances.nlargest(10).plot(kind='barh')"
      ],
      "metadata": {
        "id": "W6WkgA052odu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Split the data:\n",
        "Finally, split the dataset into training and testing sets."
      ],
      "metadata": {
        "id": "jeXuL_A019uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "VBpoJCph20hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the basic steps you can follow for data cleaning in Python before training a machine learning model. However, the actual steps you take may depend on the specific dataset and the machine learning problem you are trying to solve. More Steps might get added in future."
      ],
      "metadata": {
        "id": "498gjkZB19sf"
      }
    }
  ]
}